from llama_cpp import Llama
import os

base_path = r"C:\ai_voice_assistant\models\llm"

models = [
    "mistral-7b-instruct-v0.3.Q4_K_M.gguf",
    "meta-llama-3-8b-instruct.Q4_K_M.gguf"
]

for model_name in models:
    path = os.path.join(base_path, model_name)
    print(f"\nüîπ –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–∏: {model_name}")
    if not os.path.exists(path):
        print("‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω:", path)
        continue
    try:
        llm = Llama(model_path=path, n_ctx=1024, n_threads=8)
        print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
        del llm  # –æ—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
    except Exception as e:
        print("‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ:", e)
